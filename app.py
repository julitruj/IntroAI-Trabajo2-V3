import streamlit as st
import json
import pandas as pd
import time
from datetime import datetime
import uuid
import zipfile
import io
import os
from typing import List, Dict, Any, Optional
import PyPDF2
from llm_backend import LLMBackend, ModelConfig

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="VoC Analyst - An√°lisis de Voz del Cliente con LLM",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Inicializar el estado de la sesi√≥n
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'run_id' not in st.session_state:
    st.session_state.run_id = None
if 'uploaded_files_data' not in st.session_state:
    st.session_state.uploaded_files_data = []
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False

def extract_text_from_pdf(pdf_file) -> str:
    """Extraer texto de archivo PDF"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"Error al extraer texto de PDF: {str(e)}")
        return ""

def validate_file_size(file) -> bool:
    """Validar que el tama√±o del archivo sea menor a 100MB"""
    file.seek(0, 2)  # Mover al final del archivo
    size = file.tell()
    file.seek(0)  # Reiniciar al inicio
    return size <= 100 * 1024 * 1024  # 100MB

def process_uploaded_files(uploaded_files) -> List[Dict[str, Any]]:
    """Procesar archivos subidos y extraer contenido de texto"""
    processed_files = []
    
    for file in uploaded_files:
        if not validate_file_size(file):
            st.error(f"El archivo {file.name} excede el l√≠mite de 100MB")
            continue
            
        try:
            if file.type == "text/plain":
                content = str(file.read(), "utf-8")
            elif file.type == "application/pdf":
                content = extract_text_from_pdf(file)
            else:
                st.error(f"Tipo de archivo no soportado: {file.type}")
                continue
                
            if content.strip():
                processed_files.append({
                    "filename": file.name,
                    "content": content,
                    "size": len(content),
                    "type": file.type
                })
            else:
                st.error(f"No se encontr√≥ contenido de texto en {file.name}")
                
        except Exception as e:
            st.error(f"Error al procesar {file.name}: {str(e)}")
            
    return processed_files

def display_kpis(kpis: Dict[str, Any]):
    """Mostrar KPIs en formato de panel"""
    st.subheader("üìä Indicadores Clave de Desempe√±o")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        nps_data = kpis.get('nps', {})
        nps_value = nps_data.get('value', 0)
        st.metric(
            label="Net Promoter Score (NPS)",
            value=f"{nps_value}",
            help=f"Promotores: {nps_data.get('promoters', 0)}%, Detractores: {nps_data.get('detractors', 0)}%, Pasivos: {nps_data.get('passives', 0)}%"
        )
        if nps_data.get('simulated'):
            st.caption("*Simulado por LLM")
    
    with col2:
        csat_data = kpis.get('csat', {})
        csat_value = csat_data.get('mean', 0)
        st.metric(
            label="Satisfacci√≥n del Cliente (CSAT)",
            value=f"{csat_value:.1f}/5.0",
        )
        if csat_data.get('simulated'):
            st.caption("*Simulado por LLM")
    
    with col3:
        sentiment_data = kpis.get('sentiment', {})
        pos_avg = sentiment_data.get('pos', 0)
        st.metric(
            label="Sentimiento Positivo",
            value=f"{pos_avg:.2f}",
            help=f"Puntajes promedio de sentimiento - Negativo: {sentiment_data.get('neg', 0):.2f}, Neutral: {sentiment_data.get('neu', 0):.2f}"
        )

def display_topics(topics: List[Dict[str, Any]]):
    """Mostrar an√°lisis de temas"""
    st.subheader("üè∑Ô∏è Temas Descubiertos")
    
    if not topics:
        st.info("No se descubrieron temas en el an√°lisis.")
        return
    
    # Crear DataFrame para los temas
    topics_df = pd.DataFrame([
        {
            "ID de Tema": topic['topic_id'],
            "Etiqueta": topic['label'],
            "Palabras Clave": ", ".join(topic.get('keywords', [])),
            "Descripci√≥n": topic['description'][:100] + "..." if len(topic['description']) > 100 else topic['description']
        }
        for topic in topics
    ])
    
    st.dataframe(topics_df, width='stretch')
    
    # Detalles de temas en secciones expandibles
    st.subheader("üìù Res√∫menes de Temas")
    for topic in topics:
        with st.expander(f"Tema {topic['topic_id']}: {topic['label']}"):
            st.write(f"**Descripci√≥n:** {topic['description']}")
            st.write(f"**Palabras Clave:** {', '.join(topic.get('keywords', []))}")
            st.write(f"**Resumen:** {topic.get('summary', 'No hay resumen disponible')}")
            
            bullets = topic.get('bullets', [])
            if bullets:
                st.write("**Puntos Clave:**")
                for bullet in bullets:
                    st.write(bullet)

def display_recommendations(recommendations: List[Dict[str, Any]]):
    """Mostrar recomendaciones SMART"""
    st.subheader("üí° Recomendaciones SMART")
    
    if not recommendations:
        st.info("No se generaron recomendaciones.")
        return
    
    # Agrupar por tema
    topic_recs = {}
    for rec in recommendations:
        topic_id = rec.get('topic_id', 'Desconocido')
        if topic_id not in topic_recs:
            topic_recs[topic_id] = []
        topic_recs[topic_id].append(rec)
    
    for topic_id, recs in topic_recs.items():
        st.write(f"**Recomendaciones del Tema {topic_id}:**")
        
        for i, rec in enumerate(recs, 1):
            with st.expander(f"Recomendaci√≥n {i}: {rec.get('what', 'Sin t√≠tulo')[:50]}..."):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**Qu√©:** {rec.get('what', 'No especificado')}")
                    st.write(f"**Qui√©n:** {rec.get('who', 'No especificado')}")
                    st.write(f"**Cu√°ndo:** {rec.get('when', 'No especificado')}")
                
                with col2:
                    st.write(f"**M√©trica:** {rec.get('metric', 'No especificado')}")
                    st.write(f"**Impacto:** {rec.get('impact', 'No especificado')}")
                    
                    tag = rec.get('tag', 'Desconocido')
                    tag_colors = {
                        'quick win': 'üü¢',
                        'proceso': 'üîµ', 
                        'producto': 'üü†',
                        'formaci√≥n': 'üü°',
                        'pol√≠tica': 'üî¥'
                    }
                    st.write(f"**Etiqueta:** {tag_colors.get(tag, '‚ö™')} {tag}")

def display_message_assignments(assignments: List[Dict[str, Any]], conversations: List[Dict[str, Any]]):
    """Mostrar muestra de asignaciones de mensajes"""
    st.subheader("üí¨ Muestra de An√°lisis de Mensajes")
    
    if not assignments:
        st.info("No hay asignaciones de mensajes disponibles.")
        return
    
    # Mostrar primeras 10 asignaciones como muestra
    sample_assignments = assignments[:10]
    
    assignments_df = pd.DataFrame([
        {
            "ID de Conversaci√≥n": assign.get('conversation_id', 'Desconocido'),
            "ID de Tema": assign.get('topic_id', 'Desconocido'),
            "Sentimiento": assign.get('sentiment_label', 'Desconocido'),
            "Puntaje de Sentimiento": f"{assign.get('sentiment_score', 0):.2f}",
            "Emoci√≥n (GEW)": assign.get('familia_gew', 'Desconocido'),
            "Intensidad de Emoci√≥n": f"{assign.get('intensidad', 0)}/5",
            "Valencia": f"{assign.get('valencia', 0):.2f}"
        }
        for assign in sample_assignments
    ])
    
    st.dataframe(assignments_df, width='stretch')
    
    if len(assignments) > 10:
        st.caption(f"Mostrando 10 de {len(assignments)} asignaciones de mensajes en total. Datos completos disponibles en la exportaci√≥n.")

def export_results(results: Dict[str, Any], run_id: str):
    """Crear archivos de exportaci√≥n"""
    st.subheader("üì• Exportar Resultados")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Exportar JSON
        json_str = json.dumps(results, indent=2, ensure_ascii=False)
        st.download_button(
            label="üìÑ Descargar Resultados Completos (JSON)",
            data=json_str,
            file_name=f"voc_analysis_{run_id}.json",
            mime="application/json"
        )
    
    with col2:
        # Exportar CSV para temas
        if results.get('topics'):
            topics_df = pd.DataFrame([
                {
                    "ID de Tema": topic['topic_id'],
                    "Etiqueta": topic['label'],
                    "Descripci√≥n": topic['description'],
                    "Palabras Clave": ", ".join(topic.get('keywords', [])),
                    "Resumen": topic.get('summary', ''),
                }
                for topic in results['topics']
            ])
            
            csv_data = topics_df.to_csv(index=False)
            st.download_button(
                label="üìä Descargar Temas (CSV)",
                data=csv_data,
                file_name=f"voc_topics_{run_id}.csv",
                mime="text/csv"
            )

# Interfaz principal
st.title("üìä VoC Analyst - An√°lisis con LLM")
st.write("Transforma transcripciones de interacciones con clientes en informaci√≥n accionable usando an√°lisis avanzado con LLM")

# Barra lateral de configuraci√≥n
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # Selecci√≥n de modelo
    st.subheader("ü§ñ Selecciona el Modelo LLM")
    
    provider = st.selectbox(
        "Proveedor",
        ["OpenAI", "Anthropic", "Gemini"],
        key="provider_select"
    )
    
    # Opciones de modelo seg√∫n proveedor
    model_options = {
        "OpenAI": [
            "gpt-5",
            "gpt-4.1",
            "gpt-4.1-mini", 
            "gpt-4o",
            "o1-mini"
        ],
        "Anthropic": [
            "claude-sonnet-4-20250514",
            "claude-3-7-sonnet-20250219",
            "claude-3-5-sonnet-20241022"
        ],
        "Gemini": [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-1.5-pro",
            "gemini-1.5-flash"
        ]
    }
    
    model = st.selectbox(
        "Modelo",
        model_options[provider],
        key="model_select"
    )
    
    # Entrada de API Key (con variable de entorno como respaldo)
    api_key_labels = {
        "OpenAI": "OPENAI_API_KEY",
        "Anthropic": "ANTHROPIC_API_KEY", 
        "Gemini": "GEMINI_API_KEY"
    }
    
    api_key_env = api_key_labels[provider]
    api_key = st.text_input(
        f"Clave API de {provider}",
        type="password",
        value=os.getenv(api_key_env, ""),
        help=f"Ingrese su clave API de {provider} o configure la variable de entorno {api_key_env}"
    )
    
    if not api_key:
        st.warning(f"Por favor proporcione la clave API de {provider} para continuar")

# √Årea de contenido principal
tab1, tab2, tab3 = st.tabs(["üìÅ Subir y Procesar", "üìä Panel", "üì• Exportar"])

with tab1:
    st.header("Subir Archivos de Interacciones con Clientes")
    st.write("Suba archivos TXT o PDF que contengan conversaciones cliente-agente. Cada archivo debe contener exactamente una interacci√≥n.")
    
    # Cargador de archivos
    uploaded_files = st.file_uploader(
        "Seleccionar archivos",
        type=['txt', 'pdf'],
        accept_multiple_files=True,
        help="Suba archivos .txt o .pdf (m√°x 100MB cada uno). Cada archivo debe contener una interacci√≥n de cliente."
    )
    
    if uploaded_files:
        st.write(f"üìÅ {len(uploaded_files)} archivo(s) cargado(s)")
        
        # Procesar archivos
        if st.button("üîç Procesar Archivos", disabled=not api_key):
            with st.spinner("Procesando archivos cargados..."):
                processed_files = process_uploaded_files(uploaded_files)
                st.session_state.uploaded_files_data = processed_files
                
            if processed_files:
                st.success(f"‚úÖ {len(processed_files)} archivos procesados exitosamente")
                
                # Mostrar resumen de archivos
                files_df = pd.DataFrame([
                    {
                        "Archivo": f['filename'],
                        "Tipo": f['type'],
                        "Tama√±o (caracteres)": f['size']
                    }
                    for f in processed_files
                ])
                st.dataframe(files_df, width='stretch')

    # Mostrar bot√≥n de an√°lisis si hay archivos procesados en estado de sesi√≥n
    if st.session_state.uploaded_files_data:
        st.write("üìã Archivos listos para an√°lisis:")
        
        # Mostrar resumen de archivos procesados
        files_summary_df = pd.DataFrame([
            {
                "Archivo": f['filename'],
                "Tipo": f['type'],
                "Tama√±o (caracteres)": f['size']
            }
            for f in st.session_state.uploaded_files_data
        ])
        st.dataframe(files_summary_df, width='stretch')
        
        # Bot√≥n de an√°lisis - siempre disponible cuando hay archivos procesados
        if st.button("üöÄ Analizar con LLM", disabled=not api_key):
            processed_files = st.session_state.uploaded_files_data
            if not api_key:
                st.error("Se requiere clave API para el an√°lisis")
            else:
                # Crear configuraci√≥n del modelo
                model_config = ModelConfig(
                    provider=provider.lower(),
                    model=model,
                    api_key=api_key
                )
                
                # Inicializar backend LLM
                backend = LLMBackend(model_config)
                
                # Generar ID de ejecuci√≥n
                run_id = str(uuid.uuid4())[:8]
                st.session_state.run_id = run_id
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    # Optimizado: Llamada √∫nica al LLM para procesamiento en lote
                    status_text.text("üöÄ Procesando todos los archivos con LLM en lote (m√°s r√°pido)...")
                    progress_bar.progress(50)
                    
                    st.write(f"üî¨ Analizando {len(processed_files)} archivos en lote...")
                    
                    # Usar procesamiento en lote para mejor rendimiento
                    analysis_results = backend.analyze_conversations_batch(processed_files)
                    
                    if analysis_results and isinstance(analysis_results, dict):
                        progress_bar.progress(100)
                        status_text.text("‚úÖ ¬°An√°lisis completo!")
                        
                        # Guardar resultados
                        st.session_state.analysis_results = analysis_results
                        st.session_state.processing_complete = True
                        
                        # Mostrar resumen
                        topics_count = len(analysis_results.get('topics', []))
                        recs_count = len(analysis_results.get('recommendations', []))
                        
                        st.success(f"üéâ ¬°An√°lisis completado exitosamente! ID de ejecuci√≥n: {run_id}")
                        st.write(f"üìä Se encontraron {topics_count} temas y {recs_count} recomendaciones")
                        st.info("üëâ Revise la pesta√±a Panel para ver resultados")
                    else:
                        st.error("‚ùå Fall√≥ el an√°lisis - No se devolvieron resultados v√°lidos")
                            
                except Exception as e:
                    st.error(f"‚ùå Fall√≥ el an√°lisis: {str(e)}")
                    import traceback
                    st.text("Informaci√≥n de depuraci√≥n:")
                    st.code(traceback.format_exc())
                finally:
                    progress_bar.empty()
                    status_text.empty()

with tab2:
    st.header("Panel de An√°lisis")
    
    if st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        # Mostrar KPIs
        if 'kpis' in results:
            display_kpis(results['kpis'])
        
        st.divider()
        
        # Mostrar Temas
        if 'topics' in results:
            display_topics(results['topics'])
        
        st.divider()
        
        # Mostrar Recomendaciones
        if 'recommendations' in results:
            display_recommendations(results['recommendations'])
        
        st.divider()
        
        # Mostrar Muestra de Asignaciones de Mensajes
        if 'message_assignments' in results:
            display_message_assignments(
                results['message_assignments'],
                results.get('conversations', [])
            )
    else:
        st.info("üìà No hay resultados de an√°lisis disponibles. Por favor cargue y procese archivos primero.")

with tab3:
    st.header("Exportar Resultados")
    
    if st.session_state.analysis_results and st.session_state.run_id:
        export_results(st.session_state.analysis_results, st.session_state.run_id)
    else:
        st.info("üì¶ No hay resultados disponibles para exportar. Por favor complete un an√°lisis primero.")

# Pie de p√°gina
st.divider()
st.caption("VoC Analyst - Impulsado por tecnolog√≠a LLM para un an√°lisis integral de la voz del cliente")


