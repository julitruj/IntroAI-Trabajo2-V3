# Documentaci√≥n del Proyecto

## Gu√≠a de Usuario
# Gu√≠a de Usuario: Herramienta de An√°lisis y Procesamiento de Informaci√≥n

Esta documentaci√≥n te ayudar√° a conocer y utilizar la aplicaci√≥n dise√±ada para facilitar el an√°lisis y procesamiento de datos a trav√©s de una interfaz web interactiva. La herramienta integra funcionalidades de procesamiento de archivos PDF y an√°lisis de texto con modelos de lenguaje basados en Inteligencia Artificial utilizando Python y sus bibliotecas.

---

## Descripci√≥n de la Aplicaci√≥n

La aplicaci√≥n es una soluci√≥n integral que combina:
- **Interfaz Web Interactiva con Streamlit:** Permite la interacci√≥n en tiempo real y la visualizaci√≥n intuitiva de la informaci√≥n.
- **Procesamiento de Archivos PDF:** Facilita la carga, extracci√≥n y preprocesamiento de contenidos presentes en documentos PDF.
- **An√°lisis con Modelos de Lenguaje (IA):** Emplea algoritmos y modelos de lenguaje para interpretar y analizar el contenido extra√≠do, ayudando a extraer conclusiones o clasificar la informaci√≥n.

Esta herramienta est√° dise√±ada para usuarios que requieren analizar grandes vol√∫menes de datos en forma de documentos y obtener insights de manera r√°pida y eficiente.

---

## Funcionalidades Principales

1. **Interfaz Web Interactiva con Streamlit:**  
   - Interfaz amigable y f√°cil de usar.
   - Visualizaci√≥n en tiempo real de los resultados.
   - Acceso desde cualquier navegador web.

2. **Procesamiento de Archivos PDF:**  
   - Carga y lectura de documentos PDF.
   - Extracci√≥n de texto e informaci√≥n relevante de los archivos.
   - Preprocesamiento del texto para preparar el an√°lisis.

3. **An√°lisis con Modelos de Lenguaje (IA):**  
   - Empleo de modelos de lenguaje para interpretar el contenido textual.
   - An√°lisis sem√°ntico y de sentimientos.
   - Clasificaci√≥n de la informaci√≥n y generaci√≥n de res√∫menes autom√°ticos.

---

## C√≥mo Utilizar la Aplicaci√≥n

### 1. Acceso a la Interfaz Web
- Ingresa a la URL proporcionada (por ejemplo, http://localhost:8501 si ejecutas la aplicaci√≥n localmente).
- Se desplegar√° la interfaz de usuario basada en Streamlit.

### 2. Carga del Archivo PDF
- En la pantalla principal encontrar√°s un bot√≥n o secci√≥n para subir archivos.
- Arrastra y suelta el archivo PDF o selecciona uno desde tu sistema.
- La aplicaci√≥n validar√° y cargar√° el documento.

### 3. Procesamiento y An√°lisis
- Una vez cargado el PDF, se iniciar√° el proceso de extracci√≥n y preprocesamiento del texto.
- Despu√©s de la extracci√≥n, los modelos de lenguaje analizar√°n el contenido.
- Los resultados del an√°lisis, como res√∫menes o clasificaciones, se mostrar√°n en la interfaz.

### 4. Visualizaci√≥n de Resultados
- Los resultados se presentan en formato visual (gr√°ficos, res√∫menes, tablas, etc.).
- Podr√°s interactuar con los resultados, por ejemplo, filtrando informaci√≥n o ampliando secciones espec√≠ficas.
- Opcionalmente, ser√° posible descargar los resultados o exportarlos para su posterior uso.

### 5. Finalizar la Sesi√≥n
- Una vez terminado el an√°lisis, puedes cerrar la sesi√≥n o cargar un nuevo documento para realizar otro an√°lisis.

---

## Preguntas Frecuentes (FAQs)

**1. ¬øQu√© tipos de archivos se pueden procesar?**  
La aplicaci√≥n est√° optimizada para archivos PDF. En futuras versiones se evaluar√° la compatibilidad con otros formatos de documentos.

**2. ¬øC√≥mo se protege la privacidad de mi informaci√≥n?**  
La aplicaci√≥n procesa los documentos de forma local o en entornos seguros. Se recomienda revisar la pol√≠tica de privacidad para m√°s detalles sobre el manejo de datos.

**3. ¬øNecesito conocimientos t√©cnicos para usar la herramienta?**  
No es necesario. La interfaz est√° dise√±ada para ser intuitiva y accesible para usuarios sin experiencia t√©cnica.

**4. ¬øEs posible realizar an√°lisis en lote o procesar varios archivos a la vez?**  
La versi√≥n actual permite el procesamiento de un archivo PDF por sesi√≥n. Para an√°lisis en lote, consulta la documentaci√≥n avanzada o contacta con el soporte t√©cnico.

**5. ¬øQu√© tipo de an√°lisis de lenguaje realiza la aplicaci√≥n?**  
La herramienta utiliza modelos de lenguaje avanzados para extraer res√∫menes, interpretar sentimientos y clasificar informaci√≥n. La precisi√≥n del an√°lisis depender√° del contenido y la complejidad del documento.

**6. ¬øPuedo integrar esta herramienta en mi flujo de trabajo?**  
S√≠, la aplicaci√≥n puede integrarse en entornos que requieran an√°lisis de documentos y procesamiento de informaci√≥n. Adem√°s, dispone de endpoints que permiten la comunicaci√≥n con otros sistemas.

---

## Conclusi√≥n

Esta gu√≠a ofrece una visi√≥n general de las funciones y el uso de la herramienta de an√°lisis y procesamiento de informaci√≥n. La interfaz intuitiva, combinada con las capacidades de procesamiento de PDF y an√°lisis con IA, facilita la obtenci√≥n de insights significativos a partir de documentos complejos.

Si tienes m√°s preguntas o necesitas asistencia adicional, ¬°no dudes en ponerte en contacto con el equipo de soporte!

## Documentaci√≥n T√©cnica
# VoC Analyst ‚Äì Documentaci√≥n T√©cnica

VoC Analyst es una aplicaci√≥n orientada al an√°lisis de la Voz del Cliente (VoC). La herramienta permite la carga y procesamiento de archivos (por ejemplo, archivos PDF), la extracci√≥n de texto, el an√°lisis de conversaciones y la generaci√≥n de insights mediante Modelos de Lenguaje (LLM). La interfaz de usuario se construye con Streamlit, mientras que el backend integra m√∫ltiples proveedores LLM (como OpenAI, Anthropic y Google GenAI) para ofrecer an√°lisis avanzados.

Esta documentaci√≥n est√° dise√±ada para desarrolladores que deseen comprender, extender o mantener el sistema.

---

## Tabla de Contenidos

1. [Resumen del Repositorio](#resumen-del-repositorio)
2. [Arquitectura General](#arquitectura-general)
3. [Componentes Principales](#componentes-principales)  
   3.1 [Aplicaci√≥n Streamlit](#aplicaci√≥n-streamlit)  
   3.2 [M√≥dulo LLMBackend](#m√≥dulo-llmbackend)  
   3.3 [Extracci√≥n y Procesamiento de Archivos](#extracci√≥n-y-procesamiento-de-archivos)  
   3.4 [Parser y An√°lisis de Conversaciones](#parser-y-an√°lisis-de-conversaciones)
4. [APIs Internas y Funciones Destacadas](#apis-internas-y-funciones-destacadas)
5. [Configuraci√≥n y Dependencias](#configuraci√≥n-y-dependencias)
6. [Gu√≠as de Desarrollo](#gu√≠as-de-desarrollo)  
   6.1 [Instalaci√≥n y Ejecuci√≥n](#instalaci√≥n-y-ejecuci√≥n)  
   6.2 [Extender y Configurar LLMBackend](#extender-y-configurar-llmbackend)  
   6.3 [Pruebas y Validaci√≥n](#pruebas-y-validaci√≥n)
7. [Diagrama de Dependencias](#diagrama-de-dependencias)
8. [Consideraciones Finales](#consideraciones-finales)

---

## Resumen del Repositorio

- **Lenguajes:**  
  Se identifica el uso de m√∫ltiples lenguajes, principalmente Python, aunque se etiqueta como *other*.
  
- **Endpoints:**  
  No se han detectado endpoints expuestos, ya que la comunicaci√≥n se da a trav√©s de la interfaz Streamlit y llamadas internas a la API de cada proveedor LLM.

- **Diagrama Mermaid de Dependencias:**

  ```mermaid
  graph LR
  App[app]-->Deps[dependencies]
  ```

---

## Arquitectura General

VoC Analyst se compone de dos capas principales:

1. **Capa de Presentaci√≥n (Frontend):**  
   - Implementada en Streamlit.
   - Permite la interacci√≥n del usuario, carga de archivos, visualizaci√≥n de resultados y control del flujo de procesamiento de datos.

2. **Capa de L√≥gica y An√°lisis (Backend):**  
   - Incluye el m√≥dulo LLMBackend que se encarga de la integraci√≥n con proveedores LLM.
   - Funciones de procesamiento y an√°lisis de archivos (por ejemplo, extracci√≥n de texto de PDFs).
   - Parser y procesamiento de conversaciones para extraer insights y generar recomendaciones.

Esta separaci√≥n facilita la escalabilidad y la integraci√≥n de nuevos m√≥dulos o proveedores de LLM, permitiendo ampliar las funcionalidades sin afectar la interfaz de usuario.

---

## Componentes Principales

### 1. Aplicaci√≥n Streamlit

- **Responsabilidad:**  
  Gestionar la interfaz de usuario, el estado de la sesi√≥n y la interacci√≥n directa del usuario con la aplicaci√≥n.

- **Principales funcionalidades:**  
  - Configuraci√≥n de la p√°gina (t√≠tulo, √≠cono, layout).
  - Manejo y validaci√≥n de carga de archivos, por ejemplo, validando tama√±os de archivos y extrayendo contenido de PDFs.
  - Actualizaci√≥n del estado de la aplicaci√≥n mediante `st.session_state`.

- **Fragmento de Ejemplo:**

  ```python
  import streamlit as st
  import PyPDF2
  
  # Configuraci√≥n de la p√°gina
  st.set_page_config(
      page_title="VoC Analyst - An√°lisis de Voz del Cliente con LLM",
      page_icon="üìä",
      layout="wide",
      initial_sidebar_state="expanded"
  )
  
  # Inicializar el estado de la sesi√≥n
  if 'analysis_results' not in st.session_state:
      st.session_state.analysis_results = None
  if 'run_id' not in st.session_state:
      st.session_state.run_id = None
  if 'uploaded_files_data' not in st.session_state:
      st.session_state.uploaded_files_data = []
  if 'processing_complete' not in st.session_state:
      st.session_state.processing_complete = False
  
  def extract_text_from_pdf(pdf_file) -> str:
      """Extraer texto de archivo PDF"""
      try:
          pdf_reader = PyPDF2.PdfReader(pdf_file)
          text = ""
          for page in pdf_reader.pages:
              text += page.extract_text() + "\n"
          return text.strip()
      except Exception as e:
          st.error(f"Error al extraer texto de PDF: {str(e)}")
          return ""
  ```

### 2. M√≥dulo LLMBackend

- **Responsabilidad:**  
  Act√∫a como puente entre la aplicaci√≥n y los proveedores de Modelos de Lenguaje (LLM). Este m√≥dulo permite enviar solicitudes a la API del proveedor seleccionado (por ejemplo, OpenAI, Anthropic, Gemini de Google GenAI).

- **Componentes principales:**
  - **ModelConfig:**  
    Clase de configuraci√≥n que define el proveedor del LLM, modelo a utilizar, API key y par√°metros de reintento.
  
  - **LLMBackend:**  
    Clase que inicializa el cliente LLM seg√∫n la configuraci√≥n y prepara prompts para el parseo y an√°lisis de datos.
  
- **Fragmento de Ejemplo:**

  ```python
  import logging
  from dataclasses import dataclass
  
  # LLM SDK imports
  import openai
  from openai import OpenAI
  import anthropic
  from anthropic import Anthropic
  from google import genai
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  @dataclass
  class ModelConfig:
      """Configuration for LLM model selection"""
      provider: str  # 'openai', 'anthropic', 'gemini'
      model: str
      api_key: str
      max_retries: int = 3
      retry_delay: float = 1.0
  
  class LLMBackend:
      """Backend service for LLM-based VoC analysis"""
      
      def __init__(self, config: ModelConfig):
          self.config = config
          self.client = self._initialize_client()
          self.parse_prompt = self._load_parse_prompt()
          self.analyze_prompt = self._load_analyze_prompt()
      
      def _initialize_client(self):
          """Initialize the appropriate LLM client"""
          if self.config.provider == 'openai':
              return OpenAI(api_key=self.config.api_key)
          elif self.config.provider == 'anthropic':
              return Anthropic(api_key=self.config.api_key)
          elif self.config.provider == 'gemini':
              return genai.Client(api_key=self.config.api_key)
          else:
              raise ValueError(f"Unsupported provider: {self.config.provider}")
      
      def _load_parse_prompt(self) -> str:
          # Cargar y/o definir el prompt que se utilizar√° para parsear conversaciones
          prompt = "Incluir las instrucciones de parseo aqu√≠..."
          return prompt
      
      def _load_analyze_prompt(self) -> str:
          # Cargar y/o definir el prompt para el an√°lisis de contenido y sentimientos
          prompt = "Incluir las instrucciones de an√°lisis aqu√≠..."
          return prompt
  
      def parse_conversation(self, conversation: str) -> Dict[str, Any]:
          # Implementaci√≥n del cliente LLM para parsear el contenido de una conversaci√≥n
          logger.info("Procesando parseo de conversaci√≥n")
          # L√≥gica de reintento y manejo de peticiones a la API
          # ...
          return {"parsed": conversation}
  
      def analyze_text(self, text: str) -> Dict[str, Any]:
          # Implementaci√≥n del cliente LLM para determinar insights del texto
          logger.info("Analizando texto")
          # L√≥gica para enviar el prompt y obtener respuesta desde el LLM
          # ...
          return {"analysis": text}
  ```

### 3. Extracci√≥n y Procesamiento de Archivos

- **Responsabilidad:**  
  Gestionar la carga, validaci√≥n y extracci√≥n de contenido de archivos, centr√°ndose inicialmente en archivos PDF.
  
- **Funciones clave:**
  - `extract_text_from_pdf`: Funci√≥n para extraer el texto de cada p√°gina de un PDF.
  - `validate_file_size`: Asegura que el archivo no exceda el tama√±o permitido (100MB).

- **Consideraciones:**  
  Estas funciones permiten la normalizaci√≥n de la entrada para asegurar que el contenido sea adecuado para su posterior an√°lisis mediante LLM.

### 4. Parser y An√°lisis de Conversaciones

- **Responsabilidad:**  
  Recibir, parsear y analizar textos o di√°logos (por ejemplo, conversaciones de clientes) para extraer insights, clasificar sentimientos, identificar emociones o detectar palabras clave.

- **Flujo de trabajo:**  
  1. Recibir el contenido (ej. texto extra√≠do de un PDF o ingresado por el usuario).
  2. Utilizar un prompt predefinido para "parsear" la conversaci√≥n.
  3. Analizar el texto mediante la funci√≥n `analyze_text` del LLMBackend para obtener evaluaciones de sentimiento, tendencias o sugerencias.

---

## APIs Internas y Funciones Destacadas

Dentro de VoC Analyst se destacan las siguientes APIs y funciones:

- **LLMBackend.parse_conversation(conversation: str) -> Dict[str, Any]:**  
  Env√≠a una petici√≥n al modelo LLM para transformar y extraer la informaci√≥n estructurada de una conversaci√≥n.

- **LLMBackend.analyze_text(text: str) -> Dict[str, Any]:**  
  Procesa un bloque de texto y genera insights, tales como patrones de sentimiento o recomendaciones, a partir del an√°lisis LLM.

- **extract_text_from_pdf(pdf_file) -> str:**  
  Extrae el contenido textual completa de un archivo PDF.

- **validate_file_size(file) -> bool:**  
  Verifica que el archivo a subir cumpla con la restricci√≥n de tama√±o (menos de 100MB).

Estas funciones se pueden ampliar o modificar para adaptarse a nuevos requisitos o integrar otros proveedores de LLM.

---

## Configuraci√≥n y Dependencias

### Dependencias Principales

- **Streamlit:** Para la creaci√≥n de la interfaz web interactiva.
- **PyPDF2:** Para la lectura y extracci√≥n de texto de archivos PDF.
- **pandas:** (Opcional) Para manipulaci√≥n y an√°lisis de datos.
- **LLM SDKs:**
  - openai (OpenAI)
  - anthropic (Anthropic)
  - google.genai (Google Gemini)

### Instalaci√≥n de Dependencias

Aseg√∫rate de tener Python 3.8 o superior instalado. Usa `pip` para instalar las dependencias:

```bash
pip install streamlit pypdf2 pandas openai anthropic google-genai
```

> Nota: Revisa la documentaci√≥n de cada SDK para confirmar la versi√≥n compatible y obtener detalles de configuraci√≥n.

---

## Gu√≠as de Desarrollo

### Instalaci√≥n y Ejecuci√≥n

1. Clona el repositorio:

   ```bash
   git clone [URL_DEL_REPOSITORIO]
   cd voc-analyst
   ```

2. Instala las dependencias requeridas (ver secci√≥n de Configuraci√≥n y Dependencias).

3. Ejecuta la aplicaci√≥n en modo desarrollo:

   ```bash
   streamlit run app.py
   ```

   Esto levantar√° la interfaz web de la aplicaci√≥n en el navegador.

### Extender y Configurar LLMBackend

- Para agregar un nuevo proveedor LLM, sigue estos pasos:
  1. Actualiza la clase `ModelConfig` para permitir la especificaci√≥n del proveedor.
  2. En el m√©todo `_initialize_client()` de `LLMBackend`, a√±ade la inicializaci√≥n del cliente para el nuevo proveedor.
  3. Define o actualiza los prompts de an√°lisis seg√∫n las necesidades del nuevo modelo.

- Se recomienda implementar pruebas unitarias para validar la integraci√≥n con nuevos proveedores.

### Pruebas y Validaci√≥n

- Se deben crear pruebas que aseguren que:
  - El proceso de extracci√≥n de texto de archivos PDF funcione correctamente.
  - Las funciones de an√°lisis (`parse_conversation` y `analyze_text`) retornen respuestas conformes, incluso en casos de error.
  - El manejo de estado en la interfaz (usando `st.session_state`) se realice de forma consistente durante el flujo de procesamiento.

- Utiliza frameworks como `pytest` para automatizar la ejecuci√≥n de pruebas.

---

## Diagrama de Dependencias

A continuaci√≥n se muestra un diagrama Mermaid que resume la relaci√≥n principal entre la aplicaci√≥n y sus dependencias:

```mermaid
graph LR
  A[Aplicaci√≥n Streamlit] --> B[Procesamiento de Archivos]
  A --> C[Interfaz de Usuario]
  A --> D[LLMBackend]
  D --> E[Proveedor OpenAI]
  D --> F[Proveedor Anthropic]
  D --> G[Proveedor Gemini (Google GenAI)]
```

---

## Consideraciones Finales

- **Escalabilidad:**  
  La arquitectura modular (separaci√≥n entre interfaz y backend) permite la integraci√≥n de nuevos proveedores de LLM sin modificaciones significativas en la UI.

- **Manejo de Errores:**  
  Es fundamental implementar mecanismos robustos de manejo de errores, tanto en la extracci√≥n de texto como en la comunicaci√≥n con los servicios LLM. La funci√≥n `validate_file_size` y los bloques try/except en la funci√≥n de extracci√≥n de texto son ejemplos fundamentales de esta pr√°ctica.

- **Seguridad:**  
  Aseg√∫rate de proteger las API keys de cada proveedor LLM y evitar exponer informaci√≥n sensible en la configuraci√≥n.

- **Documentaci√≥n y Comentarios:**  
  La documentaci√≥n en el c√≥digo y en esta gu√≠a debe mantenerse actualizada para facilitar la colaboraci√≥n y el mantenimiento a largo plazo del sistema.

---

Con esta documentaci√≥n, los desarrolladores deben estar en capacidad de comprender la estructura del proyecto, sus componentes clave, y c√≥mo extender o mantener la aplicaci√≥n VoC Analyst. Para dudas adicionales o contribuciones, se recomienda revisar los comentarios en el c√≥digo y los issues en el repositorio.


## Diagrama
```mermaid
graph LR
App[app]-->Deps[dependencies]

```
