# DocumentaciÃ³n del Proyecto

## GuÃ­a de Usuario
# GuÃ­a de Usuario: Herramienta de AnÃ¡lisis y Procesamiento de InformaciÃ³n

Esta documentaciÃ³n te ayudarÃ¡ a conocer y utilizar la aplicaciÃ³n diseÃ±ada para facilitar el anÃ¡lisis y procesamiento de datos a travÃ©s de una interfaz web interactiva. La herramienta integra funcionalidades de procesamiento de archivos PDF y anÃ¡lisis de texto con modelos de lenguaje basados en Inteligencia Artificial utilizando Python y sus bibliotecas.

---

## DescripciÃ³n de la AplicaciÃ³n

La aplicaciÃ³n es una soluciÃ³n integral que combina:
- **Interfaz Web Interactiva con Streamlit:** Permite la interacciÃ³n en tiempo real y la visualizaciÃ³n intuitiva de la informaciÃ³n.
- **Procesamiento de Archivos PDF:** Facilita la carga, extracciÃ³n y preprocesamiento de contenidos presentes en documentos PDF.
- **AnÃ¡lisis con Modelos de Lenguaje (IA):** Emplea algoritmos y modelos de lenguaje para interpretar y analizar el contenido extraÃ­do, ayudando a extraer conclusiones o clasificar la informaciÃ³n.

Esta herramienta estÃ¡ diseÃ±ada para usuarios que requieren analizar grandes volÃºmenes de datos en forma de documentos y obtener insights de manera rÃ¡pida y eficiente.

---

## Funcionalidades Principales

1. **Interfaz Web Interactiva con Streamlit:**  
   - Interfaz amigable y fÃ¡cil de usar.
   - VisualizaciÃ³n en tiempo real de los resultados.
   - Acceso desde cualquier navegador web.

2. **Procesamiento de Archivos PDF:**  
   - Carga y lectura de documentos PDF.
   - ExtracciÃ³n de texto e informaciÃ³n relevante de los archivos.
   - Preprocesamiento del texto para preparar el anÃ¡lisis.

3. **AnÃ¡lisis con Modelos de Lenguaje (IA):**  
   - Empleo de modelos de lenguaje para interpretar el contenido textual.
   - AnÃ¡lisis semÃ¡ntico y de sentimientos.
   - ClasificaciÃ³n de la informaciÃ³n y generaciÃ³n de resÃºmenes automÃ¡ticos.

---

## CÃ³mo Utilizar la AplicaciÃ³n

### 1. Acceso a la Interfaz Web
- Ingresa a la URL proporcionada (por ejemplo, http://localhost:8501 si ejecutas la aplicaciÃ³n localmente).
- Se desplegarÃ¡ la interfaz de usuario basada en Streamlit.

### 2. Carga del Archivo PDF
- En la pantalla principal encontrarÃ¡s un botÃ³n o secciÃ³n para subir archivos.
- Arrastra y suelta el archivo PDF o selecciona uno desde tu sistema.
- La aplicaciÃ³n validarÃ¡ y cargarÃ¡ el documento.

### 3. Procesamiento y AnÃ¡lisis
- Una vez cargado el PDF, se iniciarÃ¡ el proceso de extracciÃ³n y preprocesamiento del texto.
- DespuÃ©s de la extracciÃ³n, los modelos de lenguaje analizarÃ¡n el contenido.
- Los resultados del anÃ¡lisis, como resÃºmenes o clasificaciones, se mostrarÃ¡n en la interfaz.

### 4. VisualizaciÃ³n de Resultados
- Los resultados se presentan en formato visual (grÃ¡ficos, resÃºmenes, tablas, etc.).
- PodrÃ¡s interactuar con los resultados, por ejemplo, filtrando informaciÃ³n o ampliando secciones especÃ­ficas.
- Opcionalmente, serÃ¡ posible descargar los resultados o exportarlos para su posterior uso.

### 5. Finalizar la SesiÃ³n
- Una vez terminado el anÃ¡lisis, puedes cerrar la sesiÃ³n o cargar un nuevo documento para realizar otro anÃ¡lisis.

---

## Preguntas Frecuentes (FAQs)

**1. Â¿QuÃ© tipos de archivos se pueden procesar?**  
La aplicaciÃ³n estÃ¡ optimizada para archivos PDF. En futuras versiones se evaluarÃ¡ la compatibilidad con otros formatos de documentos.

**2. Â¿CÃ³mo se protege la privacidad de mi informaciÃ³n?**  
La aplicaciÃ³n procesa los documentos de forma local o en entornos seguros. Se recomienda revisar la polÃ­tica de privacidad para mÃ¡s detalles sobre el manejo de datos.

**3. Â¿Necesito conocimientos tÃ©cnicos para usar la herramienta?**  
No es necesario. La interfaz estÃ¡ diseÃ±ada para ser intuitiva y accesible para usuarios sin experiencia tÃ©cnica.

**4. Â¿Es posible realizar anÃ¡lisis en lote o procesar varios archivos a la vez?**  
La versiÃ³n actual permite el procesamiento de un archivo PDF por sesiÃ³n. Para anÃ¡lisis en lote, consulta la documentaciÃ³n avanzada o contacta con el soporte tÃ©cnico.

**5. Â¿QuÃ© tipo de anÃ¡lisis de lenguaje realiza la aplicaciÃ³n?**  
La herramienta utiliza modelos de lenguaje avanzados para extraer resÃºmenes, interpretar sentimientos y clasificar informaciÃ³n. La precisiÃ³n del anÃ¡lisis dependerÃ¡ del contenido y la complejidad del documento.

**6. Â¿Puedo integrar esta herramienta en mi flujo de trabajo?**  
SÃ­, la aplicaciÃ³n puede integrarse en entornos que requieran anÃ¡lisis de documentos y procesamiento de informaciÃ³n. AdemÃ¡s, dispone de endpoints que permiten la comunicaciÃ³n con otros sistemas.

---

## ConclusiÃ³n

Esta guÃ­a ofrece una visiÃ³n general de las funciones y el uso de la herramienta de anÃ¡lisis y procesamiento de informaciÃ³n. La interfaz intuitiva, combinada con las capacidades de procesamiento de PDF y anÃ¡lisis con IA, facilita la obtenciÃ³n de insights significativos a partir de documentos complejos.

Si tienes mÃ¡s preguntas o necesitas asistencia adicional, Â¡no dudes en ponerte en contacto con el equipo de soporte!

## DocumentaciÃ³n TÃ©cnica
# VoC Analyst â€“ DocumentaciÃ³n TÃ©cnica

VoC Analyst es una aplicaciÃ³n orientada al anÃ¡lisis de la Voz del Cliente (VoC). La herramienta permite la carga y procesamiento de archivos (por ejemplo, archivos PDF), la extracciÃ³n de texto, el anÃ¡lisis de conversaciones y la generaciÃ³n de insights mediante Modelos de Lenguaje (LLM). La interfaz de usuario se construye con Streamlit, mientras que el backend integra mÃºltiples proveedores LLM (como OpenAI, Anthropic y Google GenAI) para ofrecer anÃ¡lisis avanzados.

Esta documentaciÃ³n estÃ¡ diseÃ±ada para desarrolladores que deseen comprender, extender o mantener el sistema.

---

## Tabla de Contenidos

1. [Resumen del Repositorio](#resumen-del-repositorio)
2. [Arquitectura General](#arquitectura-general)
3. [Componentes Principales](#componentes-principales)  
   3.1 [AplicaciÃ³n Streamlit](#aplicaciÃ³n-streamlit)  
   3.2 [MÃ³dulo LLMBackend](#mÃ³dulo-llmbackend)  
   3.3 [ExtracciÃ³n y Procesamiento de Archivos](#extracciÃ³n-y-procesamiento-de-archivos)  
   3.4 [Parser y AnÃ¡lisis de Conversaciones](#parser-y-anÃ¡lisis-de-conversaciones)
4. [APIs Internas y Funciones Destacadas](#apis-internas-y-funciones-destacadas)
5. [ConfiguraciÃ³n y Dependencias](#configuraciÃ³n-y-dependencias)
6. [GuÃ­as de Desarrollo](#guÃ­as-de-desarrollo)  
   6.1 [InstalaciÃ³n y EjecuciÃ³n](#instalaciÃ³n-y-ejecuciÃ³n)  
   6.2 [Extender y Configurar LLMBackend](#extender-y-configurar-llmbackend)  
   6.3 [Pruebas y ValidaciÃ³n](#pruebas-y-validaciÃ³n)
7. [Diagrama de Dependencias](#diagrama-de-dependencias)
8. [Consideraciones Finales](#consideraciones-finales)

---

## Resumen del Repositorio

- **Lenguajes:**  
  Se identifica el uso de mÃºltiples lenguajes, principalmente Python, aunque se etiqueta como *other*.
  
- **Endpoints:**  
  No se han detectado endpoints expuestos, ya que la comunicaciÃ³n se da a travÃ©s de la interfaz Streamlit y llamadas internas a la API de cada proveedor LLM.

- **Diagrama Mermaid de Dependencias:**

  ```mermaid
  graph LR
  App[app]-->Deps[dependencies]
  ```

---

## Arquitectura General

VoC Analyst se compone de dos capas principales:

1. **Capa de PresentaciÃ³n (Frontend):**  
   - Implementada en Streamlit.
   - Permite la interacciÃ³n del usuario, carga de archivos, visualizaciÃ³n de resultados y control del flujo de procesamiento de datos.

2. **Capa de LÃ³gica y AnÃ¡lisis (Backend):**  
   - Incluye el mÃ³dulo LLMBackend que se encarga de la integraciÃ³n con proveedores LLM.
   - Funciones de procesamiento y anÃ¡lisis de archivos (por ejemplo, extracciÃ³n de texto de PDFs).
   - Parser y procesamiento de conversaciones para extraer insights y generar recomendaciones.

Esta separaciÃ³n facilita la escalabilidad y la integraciÃ³n de nuevos mÃ³dulos o proveedores de LLM, permitiendo ampliar las funcionalidades sin afectar la interfaz de usuario.

---

## Componentes Principales

### 1. AplicaciÃ³n Streamlit

- **Responsabilidad:**  
  Gestionar la interfaz de usuario, el estado de la sesiÃ³n y la interacciÃ³n directa del usuario con la aplicaciÃ³n.

- **Principales funcionalidades:**  
  - ConfiguraciÃ³n de la pÃ¡gina (tÃ­tulo, Ã­cono, layout).
  - Manejo y validaciÃ³n de carga de archivos, por ejemplo, validando tamaÃ±os de archivos y extrayendo contenido de PDFs.
  - ActualizaciÃ³n del estado de la aplicaciÃ³n mediante `st.session_state`.

- **Fragmento de Ejemplo:**

  ```python
  import streamlit as st
  import PyPDF2
  
  # ConfiguraciÃ³n de la pÃ¡gina
  st.set_page_config(
      page_title="VoC Analyst - AnÃ¡lisis de Voz del Cliente con LLM",
      page_icon="ğŸ“Š",
      layout="wide",
      initial_sidebar_state="expanded"
  )
  
  # Inicializar el estado de la sesiÃ³n
  if 'analysis_results' not in st.session_state:
      st.session_state.analysis_results = None
  if 'run_id' not in st.session_state:
      st.session_state.run_id = None
  if 'uploaded_files_data' not in st.session_state:
      st.session_state.uploaded_files_data = []
  if 'processing_complete' not in st.session_state:
      st.session_state.processing_complete = False
  
  def extract_text_from_pdf(pdf_file) -> str:
      """Extraer texto de archivo PDF"""
      try:
          pdf_reader = PyPDF2.PdfReader(pdf_file)
          text = ""
          for page in pdf_reader.pages:
              text += page.extract_text() + "\n"
          return text.strip()
      except Exception as e:
          st.error(f"Error al extraer texto de PDF: {str(e)}")
          return ""
  ```

### 2. MÃ³dulo LLMBackend

- **Responsabilidad:**  
  ActÃºa como puente entre la aplicaciÃ³n y los proveedores de Modelos de Lenguaje (LLM). Este mÃ³dulo permite enviar solicitudes a la API del proveedor seleccionado (por ejemplo, OpenAI, Anthropic, Gemini de Google GenAI).

- **Componentes principales:**
  - **ModelConfig:**  
    Clase de configuraciÃ³n que define el proveedor del LLM, modelo a utilizar, API key y parÃ¡metros de reintento.
  
  - **LLMBackend:**  
    Clase que inicializa el cliente LLM segÃºn la configuraciÃ³n y prepara prompts para el parseo y anÃ¡lisis de datos.
  
- **Fragmento de Ejemplo:**

  ```python
  import logging
  from dataclasses import dataclass
  
  # LLM SDK imports
  import openai
  from openai import OpenAI
  import anthropic
  from anthropic import Anthropic
  from google import genai
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  @dataclass
  class ModelConfig:
      """Configuration for LLM model selection"""
      provider: str  # 'openai', 'anthropic', 'gemini'
      model: str
      api_key: str
      max_retries: int = 3
      retry_delay: float = 1.0
  
  class LLMBackend:
      """Backend service for LLM-based VoC analysis"""
      
      def __init__(self, config: ModelConfig):
          self.config = config
          self.client = self._initialize_client()
          self.parse_prompt = self._load_parse_prompt()
          self.analyze_prompt = self._load_analyze_prompt()
      
      def _initialize_client(self):
          """Initialize the appropriate LLM client"""
          if self.config.provider == 'openai':
              return OpenAI(api_key=self.config.api_key)
          elif self.config.provider == 'anthropic':
              return Anthropic(api_key=self.config.api_key)
          elif self.config.provider == 'gemini':
              return genai.Client(api_key=self.config.api_key)
          else:
              raise ValueError(f"Unsupported provider: {self.config.provider}")
      
      def _load_parse_prompt(self) -> str:
          # Cargar y/o definir el prompt que se utilizarÃ¡ para parsear conversaciones
          prompt = "Incluir las instrucciones de parseo aquÃ­..."
          return prompt
      
      def _load_analyze_prompt(self) -> str:
          # Cargar y/o definir el prompt para el anÃ¡lisis de contenido y sentimientos
          prompt = "Incluir las instrucciones de anÃ¡lisis aquÃ­..."
          return prompt
  
      def parse_conversation(self, conversation: str) -> Dict[str, Any]:
          # ImplementaciÃ³n del cliente LLM para parsear el contenido de una conversaciÃ³n
          logger.info("Procesando parseo de conversaciÃ³n")
          # LÃ³gica de reintento y manejo de peticiones a la API
          # ...
          return {"parsed": conversation}
  
      def analyze_text(self, text: str) -> Dict[str, Any]:
          # ImplementaciÃ³n del cliente LLM para determinar insights del texto
          logger.info("Analizando texto")
          # LÃ³gica para enviar el prompt y obtener respuesta desde el LLM
          # ...
          return {"analysis": text}
  ```

### 3. ExtracciÃ³n y Procesamiento de Archivos

- **Responsabilidad:**  
  Gestionar la carga, validaciÃ³n y extracciÃ³n de contenido de archivos, centrÃ¡ndose inicialmente en archivos PDF.
  
- **Funciones clave:**
  - `extract_text_from_pdf`: FunciÃ³n para extraer el texto de cada pÃ¡gina de un PDF.
  - `validate_file_size`: Asegura que el archivo no exceda el tamaÃ±o permitido (100MB).

- **Consideraciones:**  
  Estas funciones permiten la normalizaciÃ³n de la entrada para asegurar que el contenido sea adecuado para su posterior anÃ¡lisis mediante LLM.

### 4. Parser y AnÃ¡lisis de Conversaciones

- **Responsabilidad:**  
  Recibir, parsear y analizar textos o diÃ¡logos (por ejemplo, conversaciones de clientes) para extraer insights, clasificar sentimientos, identificar emociones o detectar palabras clave.

- **Flujo de trabajo:**  
  1. Recibir el contenido (ej. texto extraÃ­do de un PDF o ingresado por el usuario).
  2. Utilizar un prompt predefinido para "parsear" la conversaciÃ³n.
  3. Analizar el texto mediante la funciÃ³n `analyze_text` del LLMBackend para obtener evaluaciones de sentimiento, tendencias o sugerencias.

---

## APIs Internas y Funciones Destacadas

Dentro de VoC Analyst se destacan las siguientes APIs y funciones:

- **LLMBackend.parse_conversation(conversation: str) -> Dict[str, Any]:**  
  EnvÃ­a una peticiÃ³n al modelo LLM para transformar y extraer la informaciÃ³n estructurada de una conversaciÃ³n.

- **LLMBackend.analyze_text(text: str) -> Dict[str, Any]:**  
  Procesa un bloque de texto y genera insights, tales como patrones de sentimiento o recomendaciones, a partir del anÃ¡lisis LLM.

- **extract_text_from_pdf(pdf_file) -> str:**  
  Extrae el contenido textual completa de un archivo PDF.

- **validate_file_size(file) -> bool:**  
  Verifica que el archivo a subir cumpla con la restricciÃ³n de tamaÃ±o (menos de 100MB).

Estas funciones se pueden ampliar o modificar para adaptarse a nuevos requisitos o integrar otros proveedores de LLM.

---

## ConfiguraciÃ³n y Dependencias

### Dependencias Principales

- **Streamlit:** Para la creaciÃ³n de la interfaz web interactiva.
- **PyPDF2:** Para la lectura y extracciÃ³n de texto de archivos PDF.
- **pandas:** (Opcional) Para manipulaciÃ³n y anÃ¡lisis de datos.
- **LLM SDKs:**
  - openai (OpenAI)
  - anthropic (Anthropic)
  - google.genai (Google Gemini)

### InstalaciÃ³n de Dependencias

AsegÃºrate de tener Python 3.8 o superior instalado. Usa `pip` para instalar las dependencias:

```bash
pip install streamlit pypdf2 pandas openai anthropic google-genai
```

> Nota: Revisa la documentaciÃ³n de cada SDK para confirmar la versiÃ³n compatible y obtener detalles de configuraciÃ³n.

---

## GuÃ­as de Desarrollo

### InstalaciÃ³n y EjecuciÃ³n

1. Clona el repositorio:

   ```bash
   git clone [URL_DEL_REPOSITORIO]
   cd voc-analyst
   ```

2. Instala las dependencias requeridas (ver secciÃ³n de ConfiguraciÃ³n y Dependencias).

3. Ejecuta la aplicaciÃ³n en modo desarrollo:

   ```bash
   streamlit run app.py
   ```

   Esto levantarÃ¡ la interfaz web de la aplicaciÃ³n en el navegador.

### Extender y Configurar LLMBackend

- Para agregar un nuevo proveedor LLM, sigue estos pasos:
  1. Actualiza la clase `ModelConfig` para permitir la especificaciÃ³n del proveedor.
  2. En el mÃ©todo `_initialize_client()` de `LLMBackend`, aÃ±ade la inicializaciÃ³n del cliente para el nuevo proveedor.
  3. Define o actualiza los prompts de anÃ¡lisis segÃºn las necesidades del nuevo modelo.

- Se recomienda implementar pruebas unitarias para validar la integraciÃ³n con nuevos proveedores.

### Pruebas y ValidaciÃ³n

- Se deben crear pruebas que aseguren que:
  - El proceso de extracciÃ³n de texto de archivos PDF funcione correctamente.
  - Las funciones de anÃ¡lisis (`parse_conversation` y `analyze_text`) retornen respuestas conformes, incluso en casos de error.
  - El manejo de estado en la interfaz (usando `st.session_state`) se realice de forma consistente durante el flujo de procesamiento.

- Utiliza frameworks como `pytest` para automatizar la ejecuciÃ³n de pruebas.

---

## Diagrama de Dependencias

A continuaciÃ³n se muestra un diagrama Mermaid que resume la relaciÃ³n principal entre la aplicaciÃ³n y sus dependencias:

```mermaid
graph LR
  A[AplicaciÃ³n Streamlit] --> B[Procesamiento de Archivos]
  A --> C[Interfaz de Usuario]
  A --> D[LLMBackend]
  D --> E[Proveedor OpenAI]
  D --> F[Proveedor Anthropic]
  D --> G[Proveedor Gemini (Google GenAI)]
```

---

## Consideraciones Finales

- **Escalabilidad:**  
  La arquitectura modular (separaciÃ³n entre interfaz y backend) permite la integraciÃ³n de nuevos proveedores de LLM sin modificaciones significativas en la UI.

- **Manejo de Errores:**  
  Es fundamental implementar mecanismos robustos de manejo de errores, tanto en la extracciÃ³n de texto como en la comunicaciÃ³n con los servicios LLM. La funciÃ³n `validate_file_size` y los bloques try/except en la funciÃ³n de extracciÃ³n de texto son ejemplos fundamentales de esta prÃ¡ctica.

- **Seguridad:**  
  AsegÃºrate de proteger las API keys de cada proveedor LLM y evitar exponer informaciÃ³n sensible en la configuraciÃ³n.

- **DocumentaciÃ³n y Comentarios:**  
  La documentaciÃ³n en el cÃ³digo y en esta guÃ­a debe mantenerse actualizada para facilitar la colaboraciÃ³n y el mantenimiento a largo plazo del sistema.

---

Con esta documentaciÃ³n, los desarrolladores deben estar en capacidad de comprender la estructura del proyecto, sus componentes clave, y cÃ³mo extender o mantener la aplicaciÃ³n VoC Analyst. Para dudas adicionales o contribuciones, se recomienda revisar los comentarios en el cÃ³digo y los issues en el repositorio.


## Diagrama
```mermaid
graph LR
App[app]-->Deps[dependencies]

```
